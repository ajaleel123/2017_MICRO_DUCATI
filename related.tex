%!TEX root=main.tex
\section{Related Work}

\noindent While significant literature exists on improving TLB
performance, we discuss recent work that is most closely related to
the work described in this paper.

\boldheadingpara{Improving TLB Performance} Recent papers improve TLB
coverage through compression~\cite{COLT, tlbreachclustering} and
enhanced TLB organizations~\cite{SharedLLT, simTLBperf} by changing
the existing TLB structures. Other work has investigated mechanisms to
accelerate page walks by caching portions of the page
table~\cite{SkipPT,MMUcaches}, or prefetching TLB
entries~\cite{prefTLBintercore, prefTLBgokul, prefTLBrecency,
power2014supporting}, or speculating on the address translation on TLB
misses~\cite{spectlb}. When the memory footprint of workloads is
extremely large, such proposals are unable to provide full TLB
coverage and avoid TLB miss overhead. Our work focuses both on
increasing TLB coverage and reducing TLB miss penalty by storing TLB
entries in the conventional LLC and embedding a TLB in DRAM. DRAM-TLB
skips the page walk entirely and in the common case provide address
translations using a single memory access.

\boldheadingpara{Large Pages and Direct Segments} Our studies show
that the use of large pages (e.g. 2MB, 64MB, 1GB) and direct
segments~\cite{Basu2013} can significantly improve TLB coverage.
However, recent work has shown that unrestricted use of large pages
creates unintended performance
overheads~\cite{SuperPageProblem,TwoPageSize,numa-harmful,cameo,largepagevm}
causing modern operating systems to avoid using large pages in the
common case. Alternatively, the use of direct segments can improve TLB
coverage. However, direct segments requires both hardware and software
support to redesign the existing address translation system. Our
proposals on the other hand provide a complementary approach to
improve address translation using smaller pages (4KB, 64KB) without
the overhead of large pages and the design complexity of direct
segments. 

\boldheadingpara{Alternate DRAM Architectures} Recent proposals extend
the processor cache hierarchy by architecting stacked memory as a DRAM
cache~\cite{BEAR, moin2012, unison, loh2011, jaewoong2012,
dramcache-resilient}. The trade-offs for architecting DRAM-TLBs are
different from those considered in DRAM cache designs. For example,
DRAM-TLB entries are much smaller than DRAM cache entries and as such,
DRAM-TLBs expose different trade-offs for reading and updating
multiple entries without incurring higher latency or bandwidth. 
Overall, DRAM caches are orthogonal to the proposals of
this paper.

\boldheadingpara{Data Placement in Hybrid Memory} Application data
placement in hybrid memory systems as well as NUMA system has been
well studied. Hybrid memory placement policies attempt to fully
utilize total system bandwidth by distributing pages between system
memory and stacked memory based on the bandwidth
ratio~\cite{bwa,batman}. NUMA aware placement on the other hand
focuses on data placement near computing resources to minimize overall
latency~\cite{numa-traffic, numa-OSsupport, numa-bolosky}. Our work is
orthogonal these proposals.

\boldheadingpara{Address Translation Support for GPUs} Efficient and
high performing address translation on GPUs is an important area of
research. Recent studies~\cite{power2014supporting, pichaigpu} show
that simply extending CPU-style TLBs and page walkers to GPUs do not
perform well. Consequently, novel mechanisms such as highly threaded
page walkers~\cite{power2014supporting} and intelligent page walk
scheduling~\cite{pichaigpu} have been proposed. Our work focuses on
improving GPU TLB coverage by allowing the GPU LLC to hold TLB
entries. Additionally, we improve GPU TLB miss latency by architecting TLBs in DRAM.

\boldheadingpara{Inverted Page Tables} To reduce the number of memory
accesses in a hierarchical page table design, some designs use
inverted page tables~\cite{invertedPT,invertedPT2}. Inverted page
tables are designed to provide translation with a single memory access
by indexing the page table using a hash of the virtual address.
However, since different virtual addresses might produce a similar
hash, a collision-chain is created. Consequently, a drawback is that
the number of memory accesses is dependent on the length of the
collision chain. Another drawback of inverted page tables with respect
to hierarchical page tables is that they do not allow simultaneous
mapping of two different virtual addresses to the same physical
address~\cite{invertedPT}. DRAM-TLBs target the same benefits of
inverted page tables (i.e., address translation in a single memory
access) while retaining the benefits of hierarchical page tables.

% \ee{This section is pretty
% entertaining. First of all, there is no citation so its not clear what
% prior work we're even talking about. Second, it is just right out the
% gates telling the world how shitty inverted page tables are!!! I think
% a beginning sentene telling the reader why we're even talking about
% inverted page tables and also giving a citation to the related work is
% in order.} 


% DRAM-TLBs leverage the hierarchical page table structure
% and allow shared address spaces. Inverted page tables on the other
% hand do not allow shared spaces. Second, inverted page tables rely on
% a hash function to search the page table. In case of conflicts, a
% linked list must be traversed to find the translation. As such
% inverted page tables may require more than one memory access. On the
% other hand, 

% DRAM-TLBs in most common case yields a translation in a single lookup.
% However, inverted page tables first require a hash function to
% determine where to start the search, and then a linked list traversal
% to the find the corresponding mapping. Additionally, unlike inverted
% page tables, DRAM-TLBs allow flexible share of data between multiple
% virtual addresses/processes.
