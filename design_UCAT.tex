\section{Unifying Caches and TLBs}
\label{sec:UCAT}

\noindent Modern chip multiprocessors use multi-level TLB and cache
hierarchies for high performance. The last-level in each hierarchy is
architected as a single large unified structure that holds both
instruction and data entries and is shared among all cores. For
example, the unified last-level cache (LLC) contains hundreds of
thousands of cache lines\footnote{An 8MB cache with 32B lines contains
256K cache lines.}. Similarly, the unified last-level TLB (LLT)
contains 512-1024 TLB entries~\cite{}.

% that store data for recent memory references.
% that store address translations for recent memory references.

The LLT and LLC are cache structures with a tag array and a data
array. The LLT tag array stores virtual addresses while the LLC tag
array stores physical addresses.\footnote{Additional meta data (e.g.
coherence state, replacement state, etc) is also stored in the tag
array.}. The LLT data array stores the physical address corresponding
to the virtual address translation (roughly 8 bytes), while the LLC
data array stores a copy of the data from memory at a cache line
granularity (e.g. 32-64 bytes).

The LLT and LLC caches have similar sized tag arrays. Since they
provide different types of data however, the LLC has a 4x-8x larger
data array. Because of this, the LLC could potentially also serve as
gigantic TLB with as many entries as there are LLC lines.
%EIMAN-03-21: verify that each LLC line can indeed hold one LLT entry.
As such, given that last-level caches typically do not get very high
hit-rates and are often inefficiently used~\cite{}, the conventional
LLC can potentially be used to store virtual to physical translations
just like a TLB in addition to caching data from memory. Based on this
insight, we propose to re-architect the conventional LLC as a {\em
Unified Cache and TLB (UCAT)}.

\begin{figure*}[tbh] 
\vspace{-0. in}
\centering
\centerline{\psfig{file=FIGURES/UCAT,scale=0.80,width=\textwidth}}
\caption{\small UCAT Architecture. \normalsize}
\label{fig:UCAT} 
\vspace{-0.0in}
\end{figure*}

%EIMAN-03-21: My summary for reworking later: 
%   - both TLB and cache hierarchies involve multiple levels and many entries in the unified LL
%   - there is a distinction between what the tag and data of each hierarhcy holds but the cache data portion is bigger
%   - Since the tag arrays are similar in size (does this even matter?), we think of using part of the larger part of the data array of the caches to hold translation entries?
%   - Could the motivaiton here at the end be done better? Isn't the motivation really that the LLC space isn't very efficiently used in the GPU, and its the GPU translations that we care about based on the citation that we had in the intro, right?

\subsection{UCAT Architecture}

\noindent Figure~\ref{fig:UCAT}(a) illustrates a baseline system with
separate LLT and LLC structures, while Figure~\ref{fig:UCAT}(b)
demonstrates a Unified Cache and TLB (UCAT) which holds both cache
lines and TLB entries in a single hardware structure. In the latter, a
UCAT-entry is either a cacheline or a TLB-entry as determined by a
single bit in each UCAT entry. Figure~\ref{fig:UCAT}(c) shows how this
is done. When a UCAT entry stores a cache line similar to what the
baseline system does, the tag-array stores a portion of the physical
address while the data array stores the cache line (e.g.
32-bytes)~\footnote{In our baseline we assume a physically indexed,
physically tagged cache. However, any other variant of virtual or
physical indexing/tagging is equally possible with UCAT.}. However,
when the UCAT entry serves as a TLB entry, the UCAT tag-array stores
portions of the virtual address and the address space identifier
(ASID) while the data-array stores the virtual to physical address
translation. We propose to use 16 bytes of the UCAT data array for
address translation. In addition to the physical address, the
UCAT-entry also stores meta-data information such as page protection
bits and the LSB of the ASID bits\footnote{When the ASID bits do not
fit in the existing LLC tag array size, we use the top bits of the
ASID to enable a partial tag match in the tag array. The bottom bits
of the ASID are stored in the data array and are also compared to
ensure a full tag match before supplying the address translation
stored in the UCAT-entry}.

Note that even though we only use 16-bytes to store the translation,
we claim that UCAT improves upon the existing LLC inefficiency. This
is because recent studies show (and independently verified in this
study) that a majority of LLC entries are unused after cache
insertion~\cite{}. As such, UCAT utilizes the conventional LLC space
more efficiently by storing TLB entries in addition to cache lines.

UCAT enables both cachelines and TLB-entries to share space. In doing
so, both dynamically contend for the available UCAT space. Like the
baseline LLC architecture, we leverage the existing replacement policy
to manage allocating UCAT entries in a set. UCAT hits update
replacement state while UCAT misses utilize the baseline replacement
policy to select the victim.


Figure~\ref{fig:perf_UCAT} shows UCAT performance relative to the
baseline system(on the y-axis) with the x-axis illustrating the
different workloads. The first bar in the figure shows UCAT
performance where cachelines and TLB-entries contend for UCAT space
without any restrictions. The figure shows that UCAT significantly
improves performance of workloads like {\em XSBench}, {\em dmr}, {\em
MaxFlow}, and {\em MCB} by more than 2x (upto 4x in the latter two).
Other workloads like {\em GUPS}, {\em SNAP}, {\em UMT}, and {\em CoMD}
observe more than 15\% performance improvement. Overall, UCAT improves
performance across all workloads by 53\%.

To understand UCAT performance benefits, Figure~\ref{fig:tlblat_UCAT}
and Figure~\ref{fig:tlbsize_UCAT} illustrate relative translation
latency and effective on-die TLB size respectively. Workloads that
experienced more than 2x performance improvements observe a reduction
in TLB miss penalty by 70\% or more. This is because these workloads
experience an increase in on-die TLB size by 4x-64x. On average, UCAT
improves virtual to physical address translation latency by 51\% (up
to 86\% for {\em MCB}). The improvement in translation latency stems
from an increase in on-chip TLB coverage by 16x (up to 64x for {\em
dmr}).

Since UCAT decreases the effective capacity of cachelines compared to
the baseline separate LLT and LLC, we also investigated increase in
memory accesses due to potential increase in UCAT cacheline misses
relative to the baseline system. Our evaluations show that on average,
UCAT increases the number of memory accesses by 1\% (maximum 4\% for
{\em XSBench}). As observed from the performance graphs, the
additional increase in memory traffic has negligible performance
impact. This illustrates that trading off conventional LLC space for
TLB entries provides substantial performance improvements.

% \subsection{Static UCAT Partitioning}
% 
% \noindent UCAT entries must be distributed between TLB-entries and
% cachelines. One way of accomplishing this is by statically employing
% {\em way partitioning}~\cite{}. With {\em N} ways in a UCAT set, {\em
% m} ways are statically devoted to cachelines while the remaining {\em
% N-m} ways are statically devoted to TLB entries. Cachelines are only
% inserted in the ways to devoted to them while TLB entries are inserted
% in the ways devoted to them. When evicting UCAT entries, a replacement
% policy is used (e.g. LRU, RRIP) to select between candidates within
% the same UCAT partition. We refer to this design as {\em Static UCAT
% Partitioning (UCAT-S)}.
% 
% Profiling across various workloads for different values of {\em m} can
% help arrive at the best UCAT partition for cachelines and TLB entries.
% Figure X illustrates the behavior for different {\em m}.

\subsection{Managing UCAT Intelligently}

% PUCAT is inefficient when the TLB-entries and cachelines have
% different capacity requirements at run time. To address this problen,

\noindent The baseline UCAT replacement policy allocates entries based
on demand, rather than utility. As such, TLB-sensitive workloads that
frequently stream through a large number of cachelines can constantly
discard performance critical UCAT TLB entries. Based on this insight,
we enhance UCAT by improving the underlying UCAT replacement policy.
To do so, we leverage the Dynamic Re-Reference Interval Prediction
(DRRIP) replacement policy~\cite{}. In this policy, all UCAT
insertions follow the same re-reference prediction (i.e. insertion
policy). We propose to enhance the insertion policy for TLB entries by
inserting them with a {\em near-immediate prediction} rather than the
default {\em far prediction}. In doing so, TLB entries are allowed to
reside in the UCAT for a longer duration. We refer to this enhancement
as {\em UCAT with Insertion (UCAT-I)}.

Figure~\ref{fig:perf_UCAT} shows that intelligently managing UCAT by
preferencing TLB-entries over cachelines improves average performance
of UCAT by an additional 12\%. We observe that UCAT-I has negligible
increase in memory traffic (< 1\%). We observe TLB-sensitive workloads
like {\em XSBench} and {\em dmr} observe further performance
improvements (up to 90\%). Additionally, workloads like {\em LULESH},
{\em SNAP}, and {\em CoMD} observe and additional 7-10\% performance
improvement over UCAT by intelligently managing the UCAT-entries. This
is evident from Figure~\ref{fig:tlbsize_UCAT} and
Figure~\ref{fig:tlblat_UCAT} where UCAT-I increases effective TLB
coverage by nearly 2x while improving average translation latency by
7\%.

\begin{figure}[tp] 
  \vspace{-0.in} \centering
  \centerline{\psfig{file=GRAPHS/UCAT_perf,angle=-90,width=\columnwidth}}

  \caption{\small UCAT Performance. \normalsize}
  \label{fig:perf_UCAT} 
  \vspace{0.2 in}
\end{figure}

\begin{figure}[tp] 
  \vspace{0.in} \centering
  \centerline{\psfig{file=GRAPHS/UCAT_tlblat,angle=-90,width=\columnwidth}}

  \caption{\small Translation latency relative to baseline system.\normalsize}
  \label{fig:tlblat_UCAT} 
  \vspace{-0.1 in}
\end{figure}

\begin{figure}[tp] 
  \vspace{0.in} \centering
  \centerline{\psfig{file=GRAPHS/UCAT_tlbsize,angle=-90,width=\columnwidth}}

  \caption{\small Effective TLB Size.\normalsize}
  \label{fig:tlbsize_UCAT} 
  \vspace{-0.1 in}
\end{figure}


\subsection{UCAT Optimizations}

To avoid any additional storage overhead, we propose using the
UCAT-entry status bits (i.e., MESI bits) to make the distinction between 
a cache-line entry and a translation entry in the UCAT.
Specifically, we insert TLB-entries into UCAT with the {\em exclusive}
and {\em shared} status bits both set to valid. Note that cachelines can
either be in exclusive state or shared state, but not both.
